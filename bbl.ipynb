{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains functions used to forward simulate first-stage value functions using the method proposed by Bajari, Benkard and Levin (2007). The method uses conditional choice probabilities --- not estimated here --- added to random private shocks to simulate firm choice in a dynamic discrete choice.\n",
    "\n",
    "We build a very simple dynamic model with only two observed variables to analyse movie theater behavior in response to a screen quota policy. Details concerning the model are beyond the scope of this exposition. Suffice to say $x_t$ corresponds to the state variable and each $t$ represents a movie session for a movie theater in the year 2018. The algorithm works the following way (for each multiplex):\n",
    "1. At $t=1$, $x_1 = 0$. The algorithm gets week and day for $t=0$. With week information, it accesses all movies that were screened said week.\n",
    "1. Having movies, day and $x_t$ information, we get kernel density estimates for each movie according to day/$x_t$ pair. Densities of all movies are summed up, such that probabilities are given by densities relative to total. In the Logit cases, relevant observation attributes are plugged in the model to get a probability prediction.\n",
    "1. An extreme value error type I distribution is used to draw one shock for each movie.\n",
    "1. Results for (2) and (3) are added together and the highest sum determines the \"winner\" movie\n",
    "1. The expected occupation of the movie chosen in 4. is stored in an array\n",
    "1. Private shock relative to the movie chosen in 4. is also stored in an array.\n",
    "1. We record values for $\\max(0,1 - x_t)$. When $t=0$, this equals $1$.\n",
    "1. Finally, state transition is effected, according the state transition (known) function.\n",
    "1. Repeat steps $1-9$ until we reach terminal state $t=T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T14:17:14.844022Z",
     "start_time": "2021-03-14T14:17:14.208190Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# these are used to draw private shocks for each (movie) choice at each t and to introduce noise to estimates\n",
    "from scipy.stats import gumbel_r, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# função para pegar um CPB (x), ano cine, semana cine, dia absoluto e estado e devolver uma densidade,\n",
    "# a partir da densidade obtida por meio de uma kernel function calculada no arquivo principal (por isso globals)\n",
    "# DEPRECATED em favor da função falt\n",
    "\n",
    "def f_o1(x,ano,sem,dia,xt):\n",
    "    # essa é só a maneira de acessar como a variável está definida para ano semana\n",
    "    kde = globals()[f'{ano},{sem},{x}']\n",
    "    # aqui, ela bota o np.exp \"e^\" porque a função score samples retorna o log da densidade\n",
    "    return (np.exp(kde.score_samples([[dia, xt]])))[0]\n",
    "\n",
    "# função para gerar as kdes, essencialmente ela percorre as semanas dos dados e obtem KDEs não paramétricas para cada\n",
    "# obra que foi exibida naquela semana. o bandwidth foi escolhido depois de rodar algumas GridSearches com cross validation\n",
    "# de máxima verosimilhança para ver qual bandwidth era mais adequado. eu peguei a moda dos resultados de uma amostra\n",
    "# DEPRECATED\n",
    "\n",
    "def get_kdes_o1(painel,painel_obras):\n",
    "    from sklearn.neighbors import KernelDensity\n",
    "    \n",
    "    # isso é só uma lista com pares (ano, semana) porque 2017 e 2018 tem semana 52 e estão nos dados\n",
    "    semanas = list(zip([2018 for x in range(52)],[x for x in range(1,53)]))\n",
    "    semanas.insert(0, (2017,52))\n",
    "    # dic para armazenar as funções de densidade\n",
    "    d = {}\n",
    "    \n",
    "    for y,w in semanas:\n",
    "        # pega painel da semana com id, dia e fração de cumprimento (xt_frac)\n",
    "        painelzim = painel.query(\"ANO_CINEMATOGRAFICO==@y & SEMANA_CINEMATOGRAFICA==@w\")[['cpb_id','DIA_abs','xt_frac']]\n",
    "        for o in painel_obras.loc[(y,w),'cpb_id']:\n",
    "            # para cada obra nessa semana, obtem uma kde no espaço dia vs. cumprimento proporcional\n",
    "            ds = painelzim.query(\"cpb_id==@o\")[['DIA_abs','xt_frac']].to_numpy()\n",
    "            # armazenando as vars no dicionário\n",
    "            d[f'{y},{w},{o}'] = KernelDensity(bandwidth=0.52).fit(ds)\n",
    "    return d\n",
    "\n",
    "# função para devolver uma array de densidades para uma array de CPBs\n",
    "# DEPRECADA em favor da debaixo, que apenas pega o dicionário onde as densidades estão armazendas como argumento\n",
    "# ver as funções f(.) e falt(.) e get_dens()\n",
    "\n",
    "def get_dens_o1(xt, dia, sem, ano, obras):\n",
    "    # só roda uma list comprehension passando a função element-wise em todas as obras em cartaz naquela semana\n",
    "    dens = np.array([f(x, ano, sem, dia, xt) for x in obras])\n",
    "    # determina a densidade relativa: densidade sobre soma das densidades, como probabilidade e pega o log da probabilidade\n",
    "    return np.log(np.divide(dens, np.sum(dens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# função que calcula a média de #avg simulações de um complexo (#comp), usando um df #compobs com as observações de cada\n",
    "# complexo, o painel de 2018 #painel e o painel segmentado com as obras de cada semana #painel_obras,\n",
    "# DEPRECATED para c_val que toma o dicionário como argumento\n",
    "\n",
    "def cval_o1(comp, avg, compobs, painel, painel_obras, obras):\n",
    "    # pega a quantidade de observações, i.e., sessões do complexo analizado a partir de um dataframe compobs com essa info\n",
    "    obs = compobs.loc[comp]\n",
    "    #cria de antemão as arrays onde vamos armazenar os resultados de cada período simulado, nesse caso (sessoes x nº simulações)\n",
    "    # sorteio total guarda o valor das gumbels para o filme escolhido\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    # ocup total pega o valor da ocupação da sala esperada para o filme selecionado\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    # cota total pega a 1 - xt (fracionado) em cada período\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    # separa o painel para o complexo com as infos necessárias\n",
    "    # cump frac diz qual a fração da obrigação total do complexo será cumprida SE o filme escolhido for brasileiro naquela sess\n",
    "    pc_np = painel[\n",
    "            painel.REGISTRO_COMPLEXO == comp][['cump_frac','DIA_abs','SEMANA_CINEMATOGRAFICA','ANO_CINEMATOGRAFICO']].to_numpy()\n",
    "    cf, da, sc, ac = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2]), vec_int(pc_np[:,3])\n",
    "    # loopando e repetindo para a quantidade de simulações que queremos fazer para tirar a média (avg)\n",
    "    for i in range(avg):\n",
    "        # fração cumprida, começa sempre em 0\n",
    "        xt = 0\n",
    "        # ver acima para definição, mesma coisa, mas agora para 1 simulação\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        for t in range(obs):\n",
    "            # pega as obras que passaram naquela semana\n",
    "            cpb_array = painel_obras.loc[(ac[t],sc[t]),'cpb_id']\n",
    "            # pega as log probabilidades condicionais de cada obra ser escolhida\n",
    "            results = get_dens(xt, da[t], sc[t], ac[t], cpb_array)\n",
    "            # sorteia as gumbels no tamanho da quantidade de obras daquela semana\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            # calcula o vencedor vendo o máximo \n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            # registra o valor da gumbel do vencedor, que entra na função valor\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            # registra a ocupação esperada do vencedor e pega flag dizendo se ele é brasileiro\n",
    "            ocup_venc[t], flag_br = obras.loc[(ac[t],sc[t],cpb_array[vencedor]),'flag']\n",
    "            # registra cota do período\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += flag_br*cf[t]\n",
    "        # registra todos esses valores no ledger principal, para recomeçar o processo para a nova simulação\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    # já retorna a média de todas as simulações em uma array que tem (obs,3)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Disturbed value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# função para calcular o valor \"perturbado\" do problema da firma, que inclui o \"weighting factor\" wf do cara\n",
    "# essencialmente é exatamente a mesma função do cval, apenas com essa variação\n",
    "\n",
    "def distval_o1(comp, avg, compobs, painel, painel_obras, wf, obras):\n",
    "    obs = compobs.loc[comp]\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    pc_np = painel[\n",
    "            painel.REGISTRO_COMPLEXO == comp][['cump_frac','DIA_abs','SEMANA_CINEMATOGRAFICA','ANO_CINEMATOGRAFICO']].to_numpy()\n",
    "    cf, da, sc, ac = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2]), vec_int(pc_np[:,3])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        for t in range(obs):\n",
    "            cpb_array = painel_obras.loc[(ac[t],sc[t]),'cpb_id']\n",
    "            # essa é a única linha de diferença, ela transforma o array de CPBs em 1s e 0s, se brasileira ou não e soma esse \n",
    "            # viés para as obras brasileiras\n",
    "            results = get_dens(xt, da[t], sc[t], ac[t], cpb_array) + vec_bras(cpb_array)*wf\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_vencs[t], flag_br = obras.loc[(ac[t],sc[t],cpb_array[vencedor]),'flag']\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras()*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))\n",
    "\n",
    "# mesma coisa da versão anterior, com a diferença que ele pega o dicionário 'd' onde estão armazenadas as densidades!\n",
    "\n",
    "def distval_o2(comp, avg, compobs, painel, painel_obras, wf, d, obras):\n",
    "    obs = compobs.loc[comp]\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    pc_np = painel[\n",
    "            painel.REGISTRO_COMPLEXO == comp][['cump_frac','DIA_abs','SEMANA_CINEMATOGRAFICA','ANO_CINEMATOGRAFICO']].to_numpy()\n",
    "    cf, da, sc, ac = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2]), vec_int(pc_np[:,3])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        for t in range(obs):\n",
    "            cpb_array = painel_obras.loc[(ac[t],sc[t]),'cpb_id']\n",
    "            results = get_densalt(xt, da[t], sc[t], ac[t], cpb_array, d) + vec_bras(cpb_array)*wf\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t], flag_br = obras.loc[(ac[t],sc[t],cpb_array[vencedor]),'flag']\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += flag_br*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density/probability from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    }
   },
   "outputs": [],
   "source": [
    "# this functions take info about movies, dates and states and return Kernel Density estimates for each. note that they get run\n",
    "# inside the get_dens function defined below\n",
    "    # args:\n",
    "    # x = movie, given by id\n",
    "    # sem = cinematographic week\n",
    "    # dia = day\n",
    "    # xt = quota fulfilled up to time t\n",
    "    # d = dictionary storing KDEs\n",
    "\n",
    "def f(x,sem,dia,xt,d):\n",
    "    kde = d[f'{sem},{x}'] # getting KDE estimates for week/movie pair\n",
    "    return (np.exp(kde.score_samples([[dia, xt]])))[0] # scoring it according to position in day/quota fulfillment space\n",
    "\n",
    "def f_noise(x,sem,dia,xt,d): # same as above, but includes random normal error inside estimates\n",
    "    kde = d[f'{sem},{x}']\n",
    "    return (np.exp(kde.score_samples([[dia, xt]])))[0]*(1+norm.rvs(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Get functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this function creates Kernel Density Estimates used in the value functions below and return a dictionary used as argument\n",
    "# bandwidth estimates are obtained through GridSearchCV and cross validation methods\n",
    "# KDEs are calculated for each movie/week pair\n",
    "# painel = 2018 panel with all session-level observations\n",
    "# np_obras = see np_movies in cval below\n",
    "\n",
    "def get_kdes(painel,np_obras):\n",
    "    from sklearn.neighbors import KernelDensity\n",
    "    \n",
    "    d = {} # dict to store results\n",
    "    cpb_index = np.arange(np_obras.shape[0]) # check cpb_index in the cval function\n",
    "    \n",
    "    # looping over all weeks\n",
    "    for w in range(53):\n",
    "        # filter panel with all 2018 obs for week 'w' and only takes 'cpb_id', day and fractional fulfillment of quotas\n",
    "        # see pc_np argument for cval below\n",
    "        painelzim = painel.query(\"SEMANA_CINEMATOGRAFICA==@w\")[['cpb_id','DIA_abs','xt_frac']]\n",
    "        for o in cpb_index[np.where(np_obras[:,w] > 0, True, False)]:\n",
    "            # now more narrowly defined for each movie id\n",
    "            ds = painelzim.query(\"cpb_id==@o\")[['DIA_abs','xt_frac']].to_numpy()\n",
    "            # computing and storing KDEs\n",
    "            d[f'{w},{int(o)}'] = KernelDensity(bandwidth=0.52).fit(ds)\n",
    "    return d\n",
    "       \n",
    "\n",
    "# gets density using several variables and density using funcion f above, then calculates (log probability) as relative \n",
    "# density to the sum of all other movies' densities\n",
    "# xt = fractional fulfillment of screen quota obligations up to time t\n",
    "# week = cinematographic week\n",
    "# day = absolute day\n",
    "# movies = array saying which movies were on screen\n",
    "# d = dictionary with KDE results, see d above\n",
    "\n",
    "def get_dens(xt, week, day, movies, d):\n",
    "    dens = np.array([f(x, week, day, xt, d) for x in movies]) # see function f\n",
    "    return np.log(np.divide(dens, np.sum(dens))) # get log of each relative density (i.e. density / sum of densities)\n",
    "\n",
    "# this is exactly as above, but using noisy (random standard normal) errors in the density estimates\n",
    "# see distval_noise below\n",
    "\n",
    "def get_dens_noise(xt, sem, dia, obras, d):\n",
    "    dens = np.array([f_noise(x, sem, dia, xt, d) for x in obras])\n",
    "    return np.log(np.divide(dens, np.sum(dens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this function only indicates whether a id corresponds to a Brazilian or foreign movies\n",
    "\n",
    "def is_bras(x):\n",
    "    if x < 356: # unique Ancine ids start with B's for Brazilian movies and E's for foreign ones (from \"Estrangeiro\")\n",
    "                # meaning that when we order them to get simple number ids B's come first, that's why 356 is the threshold\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def vec_bras(x): # same thing but applying is bras function to a whole array at once for efficiency reasons\n",
    "    vec = np.vectorize(is_bras)\n",
    "    return vec(x)\n",
    "\n",
    "def vec_int(x): # simple vectorizing of int function\n",
    "    vec = np.vectorize(int)\n",
    "    return vec(x)\n",
    "\n",
    "def vec_zero(x): # vectorizing turning negative ints into zero, used for transforming negative remaining quotas into 0\n",
    "                 # for example if a movie theater screened 150% of its obligations 1 - x_t = - 0.5\n",
    "    vec = np.vectorize(lambda x: 0 if x < 0 else x)\n",
    "    return vec(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this function is used to transform yearly interest rates 'r' into daily discount factors to discount the value functions\n",
    "\n",
    "def daily_interest(r, days=365):\n",
    "    return (1+r/100)**(1/float(days)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Value function and counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is the main value function used, and it takes several arguments:\n",
    "# comp = movie theather complex id\n",
    "# avg = number of computations to average out, this is a requirement of BBL method to converge to the true value\n",
    "# obs = no. of movie sessions for complex 'comp' in year 2018, retrieved from data\n",
    "# pc_np = numpy array with information for each 'obs' informing (absolute) day of year, (cinematographic) week, \n",
    "    # and state transition\n",
    "# np_movies = array containing movies screened each week and info with average occupation for each one\n",
    "# d = dictionary containing Kernel Density Estimates for each movie in the time, x_t space for each week, used to\n",
    "    # compute first stage Conditional Choice Probabilities\n",
    "\n",
    "def cval(comp, avg, obs, pc_np, np_movies, d):\n",
    "    # first we create arrays to store private shocks, average occupation for the chosen movie and quota fulfilled for each\n",
    "    # period t, and for every simulation done as determined by the number to average out\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    # as stated above, we store each column of pc_np\n",
    "    # cf = fractional fulfillment of quotas, should the chosen movie in session t be Brazilian\n",
    "    # da = absolute day\n",
    "    # sc = cinematographic week\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    # movie ids for all movies have been transformed from a string to a number from 0 to 908, see below\n",
    "    cpb_index = np.arange(np_movies.shape[0])\n",
    "    # looping over the process for the number of times chosen as 'avg' argument\n",
    "    for i in range(avg):\n",
    "        # quota fulfillment starts at 0%\n",
    "        xt = 0\n",
    "        # here we create arrays to store results same as above, but for the present simulation\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        # to see movie ids displayed each week, we start with week 0 of 'np_movies[:,0]' and look only for the rows\n",
    "        # where values are not 0. in other words, np_movies stores in each columns information regarding all movies screened\n",
    "        # noting 0 if the movie was not on screens that week and the expected occupation value otherwise. rows are indexed\n",
    "        # from 0 to 908. 'cpb_index' is constructed to rebuild this index in the np.array object\n",
    "        cpb_array = cpb_index[np.where(np_movies[:,0] > 0, True, False)]\n",
    "        for t in range(obs):\n",
    "            # this only changes the movies when the week changes as we go through the t sessions\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    cpb_array = cpb_index[np.where(np_movies[:,sc[t]] > 0, True, False)]\n",
    "            # results computes log probabilities for each movie on screen, as recorded by 'cpb_array'\n",
    "            # for details, see helper function get_dens in the helper functions\n",
    "            # arguments include state, week, day, besides movie and the stored dictionary of first stage estimators\n",
    "            results = get_dens(xt, sc[t], da[t], cpb_array, d)\n",
    "            # random shocks are drawn from extreme type I value distribution for each movie (thus the shape)\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            # chosen movie is determined by the max of results and random shocks\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            # now we store occupation, shock and screen quota information with the chosen movie\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_movies[cpb_array[vencedor],sc[t]]\n",
    "            # see that the chosen movie does not affect quota fulfillment in state t, only in t+1\n",
    "            cotas[t] = 1 - xt\n",
    "            # function is_bras checks if the movie is Brazilian according to its unique id, returning 1 if True\n",
    "            # to get state transition we only look for fractional fulfillment previously calculated\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        # now we record full results of 1 round of simulation in the initial array\n",
    "        # note that vec_zero now transforms all negative remaining quota values to 0\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    # return the average of each column\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is essentially the same as above, but now we use logit first stage CCP estimators, which requires some changes\n",
    "# Only changes are noted. For omitted details, check cval above.\n",
    "# Different args are:\n",
    "# regs = dictionary with fitted logit models for each week (using sklearn.LogisticRegression)\n",
    "# cols = columns index for logit explanatory variables (models have interacted values and many fixed-effects so we need an\n",
    "    # index to get rid of dataframes that require more memory usage)\n",
    "\n",
    "def cval_logit(comp, avg, obs, pc_np, np_obras, regs, cols):\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        w_reg = regs['reg_0'] # choosing week 0 fitted model from dict\n",
    "        w_col = cols['semana_0'].values # same for column index\n",
    "        cpb_array = w_reg.classes_ # movies each week are dependent variables of multinomial logit stored under classes\n",
    "        # attribute of sklearn.LogisticRegression.fit()\n",
    "        for t in range(obs):\n",
    "            # same mechanism as before, rewriting variables when week changes\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    w_reg = regs[f'reg_{sc[t]}']\n",
    "                    w_col = cols[f'semana_{sc[t]}'].values\n",
    "                    cpb_array = w_reg.classes_\n",
    "            # here we prepare the array of explanatory variables to feed the logit model, using day, movie theater id and frac\n",
    "            # fulfillment\n",
    "            log_proba = np.select(\n",
    "            [w_col == f'DIA_abs_{da[t]}', w_col == f'REGISTRO_COMPLEXO_{comp}', w_col == f'REGISTRO_COMPLEXO_{comp}:xt_frac', w_col == f'DIA_abs_{da[t]}:xt_frac'],\n",
    "                [1,1,xt,xt])\n",
    "            # reshaping for the expected shape of sklearn\n",
    "            log_proba = log_proba.reshape(1,log_proba.shape[0])\n",
    "            log_proba[0,0] = xt\n",
    "            # same as cval, now we use predict log proba method directly from sklearn for all movies\n",
    "            results = w_reg.predict_log_proba(log_proba).flatten()\n",
    "            sorteio = gumbel_r.rvs(size=results.shape)\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_obras[cpb_array[vencedor],sc[t]]\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# conterfactual functions do the same as the above functions, but now they simulate paths using parameter estimates\n",
    "# obtained from second stage estimation using the BBL algorithm\n",
    "\n",
    "def counterfactual(comp, avg, pc_np, np_obras, theta_1):\n",
    "    xts = np.zeros((avg,))\n",
    "    cf, sc = pc_np[:,0], vec_int(pc_np[:,2])\n",
    "    cpb_index = np.arange(np_obras.shape[0])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        for s in range(53):\n",
    "            ocup_obras = np_obras[:,s][np.where(np_obras[:,s] > 0, True, False)]\n",
    "            cpb_array = cpb_index[np.where(np_obras[:,s] > 0, True, False)]\n",
    "            obs_semana = np.sum(np.where(sc == s, True, False))\n",
    "            sorteio = gumbel_r.rvs(size=(obs_semana, ocup_obras.shape[0]))\n",
    "            \n",
    "            ocs = np.zeros(sorteio.shape)\n",
    "            ocs[:,:] = np.multiply(theta_1, ocup_obras)\n",
    "            \n",
    "            resultados = np.sum([ocs, sorteio], axis=0)\n",
    "            idx_vencedores = np.argmax(resultados, axis=1)\n",
    "            vencedores = cpb_array[idx_vencedores]\n",
    "            \n",
    "            xt += np.sum(np.multiply(vec_bras(vencedores), cf[:obs_semana]))\n",
    "        xts[i] = xt\n",
    "    return xts.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Disturbed value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Bajari, Benkard and Levin required disturbed value functions to obtain parameter estimates in the second stage\n",
    "# the idea is that true parameters will minimize equilibrium violations, i.e., when disturbed value functions\n",
    "# yield higher equilibrium values than the true ones\n",
    "# this function only takes in a new argument:\n",
    "# wf = weighting factor, used to systematically bias log probs of brazilian movies upwards or downwards\n",
    "# for explanations, check cval function\n",
    "\n",
    "def distval(comp, avg, obs, pc_np, np_obras, wf, d):\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    cpb_index = np.arange(np_obras.shape[0])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        cpb_array = cpb_index[np.where(np_obras[:,0] > 0, True, False)]\n",
    "        for t in range(obs):\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    cpb_array = cpb_index[np.where(np_obras[:,sc[t]] > 0, True, False)]\n",
    "            # this is the only difference, brazilian movies picked by vec_bras as 1 are multiplied to bias log probs\n",
    "            results = get_dens(xt, sc[t], da[t], cpb_array, d) + vec_bras(cpb_array)*wf\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_obras[cpb_array[vencedor],sc[t]]\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))\n",
    "\n",
    "# same idea as the function before, but altering for logit first-stage estimators\n",
    "\n",
    "def distval_logit(comp, avg, obs, pc_np, np_obras, wf, regs, cols):\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        w_reg = regs['reg_0']\n",
    "        w_col = cols['semana_0'].values\n",
    "        cpb_array = w_reg.classes_\n",
    "        for t in range(obs):\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    w_reg = regs[f'reg_{sc[t]}']\n",
    "                    w_col = cols[f'semana_{sc[t]}'].values\n",
    "                    cpb_array = w_reg.classes_\n",
    "            log_proba = np.select(\n",
    "            [w_col == f'DIA_abs_{da[t]}', w_col == f'REGISTRO_COMPLEXO_{comp}', w_col == f'REGISTRO_COMPLEXO_{comp}:xt_frac', w_col == f'DIA_abs_{da[t]}:xt_frac'],\n",
    "                [1,1,xt,xt])\n",
    "            log_proba = log_proba.reshape(1,log_proba.shape[0])\n",
    "            log_proba[0,0] = xt\n",
    "            results = w_reg.predict_log_proba(log_proba).flatten() + vec_bras(cpb_array)*wf\n",
    "            sorteio = gumbel_r.rvs(size=results.shape)\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_obras[cpb_array[vencedor],sc[t]]\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:47:08.262278Z",
     "start_time": "2021-03-07T21:47:08.189869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this is a second alternative to produce disturbed value functions. in this case que just add random noise to \n",
    "# log prob estimates. essentially the functions multiplies each log prob estimate by 1+ standard random noise\n",
    "\n",
    "def distval_noise(comp, avg, obs, pc_np, np_obras, d):\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    cpb_index = np.arange(np_obras.shape[0])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        cpb_array = cpb_index[np.where(np_obras[:,0] > 0, True, False)]\n",
    "        for t in range(obs):\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    cpb_array = cpb_index[np.where(np_obras[:,sc[t]] > 0, True, False)]\n",
    "            # the only change is mediated by this different get_density function that introduces normal errors\n",
    "            results = get_dens_noise(xt, sc[t], da[t], cpb_array, d)\n",
    "            sorteio = gumbel_r.rvs(size=results.shape[0])\n",
    "            vencedor = np.argmax(results+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_obras[cpb_array[vencedor],sc[t]]\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))\n",
    "\n",
    "# exactly the same as before but for logit CCPs\n",
    "\n",
    "def distval_noise_logit(comp, avg, obs, pc_np, np_obras, regs, cols):\n",
    "    sorteio_total = np.zeros((obs,avg))\n",
    "    ocup_total = np.zeros((obs,avg))\n",
    "    cota_total = np.ones((obs,avg))\n",
    "    cf, da, sc = pc_np[:,0], vec_int(pc_np[:,1]), vec_int(pc_np[:,2])\n",
    "    for i in range(avg):\n",
    "        xt = 0\n",
    "        sort_venc = np.zeros((obs,))\n",
    "        ocup_venc = np.zeros((obs,))\n",
    "        cotas = np.ones((obs,))\n",
    "        w_reg = regs['reg_0']\n",
    "        w_col = cols['semana_0'].values\n",
    "        cpb_array = w_reg.classes_\n",
    "        for t in range(obs):\n",
    "            if t > 0:\n",
    "                if sc[t] > sc[t-1]:\n",
    "                    w_reg = regs[f'reg_{sc[t]}']\n",
    "                    w_col = cols[f'semana_{sc[t]}'].values\n",
    "                    cpb_array = w_reg.classes_\n",
    "            log_proba = np.select(\n",
    "            [w_col == f'DIA_abs_{da[t]}', w_col == f'REGISTRO_COMPLEXO_{comp}', w_col == f'REGISTRO_COMPLEXO_{comp}:xt_frac', w_col == f'DIA_abs_{da[t]}:xt_frac'],\n",
    "                [1,1,xt,xt])\n",
    "            log_proba = log_proba.reshape(1,log_proba.shape[0])\n",
    "            log_proba[0,0] = xt        \n",
    "            results = w_reg.predict_log_proba(log_proba).flatten()\n",
    "            # once again, this is the only difference\n",
    "            noise = np.multiply(results, 1+norm.rvs(size=results.shape))\n",
    "            sorteio = gumbel_r.rvs(size=results.shape)\n",
    "            vencedor = np.argmax(noise+sorteio)\n",
    "            sort_venc[t] = sorteio[vencedor]\n",
    "            ocup_venc[t] = np_obras[cpb_array[vencedor],sc[t]]\n",
    "            cotas[t] = 1 - xt\n",
    "            xt += is_bras(cpb_array[vencedor])*cf[t]\n",
    "        sorteio_total[:,i], ocup_total[:,i], cota_total[:,i] = sort_venc, ocup_venc, vec_zero(cotas)\n",
    "    return np.column_stack((sorteio_total.mean(axis=1), ocup_total.mean(axis=1), cota_total.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T17:35:17.243347Z",
     "start_time": "2021-03-14T17:35:15.518376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook bbl.ipynb to script\n",
      "[NbConvertApp] Writing 24470 bytes to bbl.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script bbl.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
